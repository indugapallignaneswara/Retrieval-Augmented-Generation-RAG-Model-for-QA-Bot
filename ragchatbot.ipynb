{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This notebook demonstrates how to build an interactive Conversational Chatbot using the LangChain framework, OpenAI's GPT-3.5-turbo, and Pinecone for vector storage. The chatbot engages in conversations with users, retrieves relevant information from a vector store, and answers questions based on the context provided by previous interactions."
      ],
      "metadata": {
        "id": "kqHynDnF17HC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing requirements"
      ],
      "metadata": {
        "id": "9mfp9B2-1W-u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R20oWk56RZix",
        "outputId": "ce15c39f-8ace-4fd1-8606-b9083fca1aba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.9/386.9 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.8/244.8 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.9/49.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.5/294.5 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.2/325.2 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m407.7/407.7 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.7/296.7 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU \\\n",
        "    langchain \\\n",
        "    openai \\\n",
        "    pinecone-client \\\n",
        "    langchain-openai \\\n",
        "    langchain-pinecone \\\n",
        "    python-dotenv \\\n",
        "    pypdf \\\n",
        "    langchain_community"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Environment Setup: Create a .env file in the runtime. The dotenv library is used to load API keys and other environment variables, ensuring a secure configuration for accessing OpenAI and Pinecone services."
      ],
      "metadata": {
        "id": "XaHwfqAg1L2g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "OPENAI_API_KEY=your_openai_api_key\n",
        "PINECONE_API_KEY=your_pinecone_api_key\n",
        "INDEX_NAME=your_pinecone_index_name\n",
        "PINECONE_CLOUD=aws  # (or any other supported cloud provider)\n",
        "PINECONE_REGION=us-east-1  # (or the region where your Pinecone project is hosted)"
      ],
      "metadata": {
        "id": "PSloVTynyLv-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**OpenAIEmbeddings** are used to convert text into embeddings (vector representations), which can be compared to determine semantic similarity between queries and stored data.\n",
        "\n",
        "**PineconeVectorStore** acts as a vector database that stores the embeddings and retrieves relevant chunks of information based on user queries."
      ],
      "metadata": {
        "id": "26TLmM_J0rrs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "def load_pdf_from_terminal():\n",
        "    \"\"\"\n",
        "    Prompts the user to input a PDF file path via the terminal.\n",
        "    Returns the file path of the PDF.\n",
        "    \"\"\"\n",
        "    pdf_path = input(\"Please enter the full path to the PDF file: \")\n",
        "\n",
        "    # Check if the file exists\n",
        "    if not os.path.exists(pdf_path):\n",
        "        raise ValueError(\"The specified file does not exist. Please enter a valid file path.\")\n",
        "\n",
        "    return pdf_path\n",
        "\n",
        "def process_pdf_file(pdf_path):\n",
        "    \"\"\"\n",
        "    Loads and processes the PDF file by splitting it into text chunks.\n",
        "    Then creates embeddings and stores them in Pinecone.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load the PDF document\n",
        "        loader = PyPDFLoader(pdf_path)\n",
        "        document = loader.load()\n",
        "\n",
        "        # Split the document into chunks\n",
        "        text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "        texts = text_splitter.split_documents(document)\n",
        "        print(f\"Created {len(texts)} chunks\")\n",
        "\n",
        "        # Generate embeddings\n",
        "        embeddings = OpenAIEmbeddings(openai_api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
        "\n",
        "        # Store the vectors in Pinecone\n",
        "        PineconeVectorStore.from_documents(texts, embeddings, index_name=os.environ.get(\"INDEX_NAME\"))\n",
        "        print(\"Embeddings have been successfully stored in Pinecone.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during processing: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        # Load PDF file from terminal\n",
        "        pdf_file_path = load_pdf_from_terminal()\n",
        "        print(f\"Selected file: {pdf_file_path}\")\n",
        "\n",
        "        # Process the loaded PDF file\n",
        "        process_pdf_file(pdf_file_path)\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQE9BY7USM0c",
        "outputId": "233c16b0-ded9-4270-d9fc-afd6d22bb7df"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please enter the full path to the PDF file: /content/Harry Potter - Book 5 - The Order of the Phoenix.pdf\n",
            "Selected file: /content/Harry Potter - Book 5 - The Order of the Phoenix.pdf\n",
            "Created 881 chunks\n",
            "Embeddings have been successfully stored in Pinecone.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chatbot is powered by LangChain's ChatOpenAI (a wrapper around **OpenAI's gpt-3.5-turbo** model). It processes the conversational inputs in real-time.\n",
        "**ConversationalRetrievalChain** is used to manage retrieval-augmented conversations, combining the power of OpenAI's language models and Pinecone's vector search to return dynamic, contextually-relevant answers."
      ],
      "metadata": {
        "id": "ljeFDrER1izk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import warnings\n",
        "from dotenv import load_dotenv\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain_community.chat_models import ChatOpenAI\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Initialize chat history\n",
        "chat_history = []\n",
        "\n",
        "def start_conversational_chatbot():\n",
        "    \"\"\"\n",
        "    Starts the chatbot in interactive mode, dynamically answering questions until the user ends the conversation.\n",
        "    \"\"\"\n",
        "    # Initialize embeddings and vector store\n",
        "    embeddings = OpenAIEmbeddings(openai_api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
        "    vectorstore = PineconeVectorStore(\n",
        "        index_name=os.environ[\"INDEX_NAME\"], embedding=embeddings\n",
        "    )\n",
        "\n",
        "    # Initialize the LLM model for chat\n",
        "    chat = ChatOpenAI(verbose=True, temperature=0, model_name=\"gpt-3.5-turbo\")\n",
        "\n",
        "    # Initialize ConversationalRetrievalChain\n",
        "    qa = ConversationalRetrievalChain.from_llm(\n",
        "        llm=chat, chain_type=\"stuff\", retriever=vectorstore.as_retriever()\n",
        "    )\n",
        "\n",
        "    print(\"Chatbot is ready. Type 'end' to finish the conversation.\")\n",
        "\n",
        "    while True:\n",
        "        # Get user input (question)\n",
        "        user_input = input(\"\\nYou: \")\n",
        "\n",
        "        # If user types \"end\", break the loop\n",
        "        if user_input.lower().strip() == \"end\":\n",
        "            print(\"Ending conversation. Goodbye!\")\n",
        "            break\n",
        "\n",
        "        # Generate the response from the chatbot\n",
        "        res = qa({\"question\": user_input, \"chat_history\": chat_history})\n",
        "\n",
        "        # Retrieve the response and print it\n",
        "        answer = res[\"answer\"]\n",
        "        print(f\"Bot: {answer}\")\n",
        "\n",
        "        # Save the current conversation to the chat history\n",
        "        chat_history.append((user_input, answer))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    start_conversational_chatbot()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_R0Epig6pHvw",
        "outputId": "723c7910-8138-431c-815e-122463b7e5ab"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chatbot is ready. Type 'end' to finish the conversation.\n",
            "\n",
            "You: Who is Harry Potter?\n",
            "Bot: Harry Potter is the main character in the Harry Potter book series written by J.K. Rowling. He is a young wizard who attends Hogwarts School of Witchcraft and Wizardry and goes on various adventures throughout the series.\n",
            "\n",
            "You: How many books are in this collection?\n",
            "Bot: There are a total of seven books in the Harry Potter book series written by J.K. Rowling.\n",
            "\n",
            "You: When did the 3rd  book published?\n",
            "Bot: The 3rd book in the Harry Potter series, \"Harry Potter and the Prisoner of Azkaban,\" was published in 1999.\n",
            "\n",
            "You: What are the other works of J. K. Rowling?\n",
            "Bot: J.K. Rowling is also known for writing \"The Casual Vacancy\" and the Cormoran Strike series under the pseudonym Robert Galbraith.\n",
            "\n",
            "You: Is Game of Thones directed by J.K. Rowling?\n",
            "Bot: No, \"Game of Thrones\" is not directed by J.K. Rowling. J.K. Rowling is the author of the Harry Potter series, while \"Game of Thrones\" is a television series based on the book series \"A Song of Ice and Fire\" by George R.R. Martin.\n",
            "\n",
            "You: tell me about 'Game of Thones'\n",
            "Bot: I don't know.\n",
            "\n",
            "You: Who is the prime minister of Pakistan?\n",
            "Bot: I don't know.\n",
            "\n",
            "You: end\n",
            "Ending conversation. Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E4HQRsv0qMh_"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2tGUJa_R4NWd"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t72T1y4J4Tm5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}